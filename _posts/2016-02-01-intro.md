---
title: "DLMHS 2018"
bg: white
color: black
style: left
fa-icon: list-ul
layout: default
icon-title: true
---
  
# DLMHS 2018

<div style="text-align:center;">
  <span class="fa-stack subtlecircle" style="font-size:64px; background:rgba(0,128,0,0.1)">
    <i class="fa fa-circle fa-stack-2x text-white"></i>
    <i class="fa fa-server fa-stack-1x text-green"></i>
  </span>
</div>

## June 12, 2018, Beijing, China.

<div style="text-align:center;">
  <a href="http://ics2018.ict.ac.cn/"><img width="960px" src="img/ICS-18-logo-1.png"/></a>
  &nbsp;  &nbsp;  &nbsp;  &nbsp;
</div>

#### Held in conjunction with the 32nd ACM International Conference on Supercomputing, ([ACM ICS-2018](http://ics2018.ict.ac.cn/)), June 12-15, 2018, Beijing, China.

<div style="text-align:center;">
  <p>
  <font style="color:red;font-size:18pt;font-face:bold;">
  Submission deadline for abstract is May 1st, 2018 (AOE)
  </font>
  </p>
</div>

### Overview
The last decade has seen blooming emergence in deep learning. With ever-increased problem size to resolve and data size to process, deep learning becomes extremely computation demanding. As a result, migrating deep learning algorithms and applications on modern supercomputers, especially heterogeneous supercomputers that incorporate special accelerators such as GPUs, Xeon-Phi, FPGA, TPU, ASIC, etc. becomes a trend. 


<div style="text-align:center;">
  <p>
    <a href="dlmhs-cfp.txt">
      <i class="fa fa-file-text-o">&nbsp;<b>Download the DLMHS 18 CFP </b></i>
    </a>
  </p>
</div>

## Topics

This workshop will emphasize novel, disruptive research ideas over incremental 
advances. We will solicit papers on topics including, but not limited to, the 
following areas:

In this workshop, we invite novel and recent works including, but not limited to, the following topics:
* Scalable deep learning algorithms and application 
* Heterogeneous programming models and interfaces for deep learning
* Special compilation techniques for deep-learning on heterogeneous supercomputers
* Practical experience and evaluations on accelerator interconnects (GPU, FPGA, Xeon-Phi)
* Memory and Cache optimization techniques for deep learning applications
* Power reduction techniques for deep-learning applications
* Performance Modeling for deep learning applications on supercomputers
* Potential Opportunities for HPC from deep learning type of applications, e.g., gradient-descent enabled approximate computing
* Potential Challenges for scaling current deep learning algorithms on supercomputers (e.g., communication bottlenecks, load balancing, etc)


## Submissions

As a 1st time workshop, we decide to make it discussion oriented this year. We invite 2-page double-column submission on novel and recent published works (kindly follow the ACM proceeding sigconf template(https://www.acm.org/publications/proceedings-template) using 10-point font). We also welcome unfinished novel ideas. Submission will not appear in proceedings so it can be further developed and submitted to a formal conference or journals. Finished or published works will be given 25 minutes to fully describe their contributions while unfinished work and novel ideas will be given 10 minutes to motivate the audience. 

Submission will be accepted through the EasyChair System through this link: ([https://easychair.org/conferences/?conf=dlmhs18](https://easychair.org/conferences/?conf=dlmhs18))

## Important Dates

* Abstract (2 pages max):  
* Deadline: May. 1st, 2018 (AOE)
* Author notification: May 13th, 2018 (AOE)

All dates are Anywhere on Earth (AOE)

## General Chairs

* Ang Li, Pacific Northwest National Laboratory
* Jidong Zhai, Lawrence Livermore National Laboratory

## Program Committee

* Shuaiwen Leon Song, Pacific Northwest National Laboratory, USA
* Xu Liu, College of William and Mary, USA
* Weifeng Liu, Norwegian University of Science and Technology, Norway
* Guoyang Chen, Qualcomm, USA
* Jiajia Li, Georgia Tech University, USA
* Dingwen Tao, Brookhaven National Laboratory, USA
* Shuai Che, AMD Research, USA 
* Qiang Guan, Kent State University, USA
* Yun Liang, Peking University, China
* Akash Kumar, Dresden University of Technology, Germany
* Wenfeng Zhao, University of Minnesota, USA
* Biao Sun, Tianjin University, China




## Workshop Program

TBA

## Keynote

TBA
